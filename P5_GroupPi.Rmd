---
title: "P5 GroupPi"
geometry: margin=0.9in
output:
  pdf_document:
    highlight: tango
    toc_depth: 2
  word_document:
    toc_depth: '2'
subtitle: CS4831
fontsize: 11pt
---
Group Pi  
Members: Jordan DeYonker, Lydia Savatsky, and Alan Bouwman

Packages used in this assignment:
```{r packages, message = FALSE, warning = FALSE, attr.source='.numberLines'}
library(ggplot2)
library(tm)
library(stopwords)
library(caret)
```

#Q4
Reading in the college data.
```{r, eval = TRUE}
college <- read.csv("p5-data/college_data.csv")
#View(college)
```
Center and Scaling Data
```{r, eval = TRUE}
col <- preProcess(college, method = c("center", "scale"))
col.scale <- predict(col, college)
```

## Q4 (a)
Performing PCA on college data.
```{r, eval = TRUE}
pc.college <- princomp(col.scale[4:21])
pc.college$loadings
```

## Q4 (b)
Plotting the data in the space defined by the first two Pcs.
```{r, eval = TRUE}
plot(pc.college$scores[,1:2], xlab="Xv_1", ylab="Xv_2", main="First two principal component scores")
text(pc.college$scores[,1]+0.25, pc.college$scores[,2], labels=col.scale[,3])
```

## Q4 (c) 
Plotting the amount of variance explained
```{r, eval = TRUE}
pve.college = pc.college$sdev^2/sum(pc.college$sdev^2)
plot(pve.college, xlab = "Principal Components", ylab = "Proportion of Variance Explained", ylim = c(0,1), type='b')

plot(cumsum(pve.college), xlab = "Principal Components", ylab = "Proportion of Cumulative Proportion of Variance Explained", ylim = c(0,1), type='b')

```
We will use an 85% threshold to decide how many components to use. Therefore, 4 principle components should be used for further analysis on the data.

# Q5
Reading in the stock data.
```{r, eval = TRUE}
stock19 <- read.csv("p5-data/stock_data_2019.csv")
stock20 <- read.csv("p5-data/stock_data_2020.csv")
#View(stock19)
#View(stock20)
```
Center and Scale Data
```{r}
s19 <- preProcess(stock19, method = c("center", "scale"))
stock19.scale <- predict(s19, stock19)

s20 <- preProcess(stock20, method = c("center", "scale"))
stock20.scale <- predict(s20, stock20)
```

## Q5 (a)
Performing PCA on DOW Jones 2020 data.
```{r, eval = TRUE}
pc20 <- princomp(stock20.scale[2:31])
pc20$loadings
```

## Q5 (b)
Plotting the data in the space defined by the first two PCs.
```{r, eval = TRUE}
plot(pc20$scores[,1:2], xlab="Xv_1", ylab="Xv_2", main="First two principal component scores")
text(pc20$scores[,1]+0.25, pc20$scores[,2], labels=stock20.scale[,1])
```

## Q5 (c)
The points seem to be roughly clustered by the dates. Earlier months appear in the lower right hand corner while later months appear in the top right hand corner. Middle months such as May through July appear in the middle of the graph.

## Q5 (d)
Performing PCA on DOW Jones 2019 data.
```{r, eval = TRUE}
pc19 <- princomp(stock19.scale[2:31])
pc19$loadings

plot(pc19$scores[,1:2], xlab="Xv_1", ylab="Xv_2", main="First two principal component scores")
text(pc19$scores[,1]+0.25, pc19$scores[,2], labels=stock19.scale[,1])
```
For the 2019 plot, earlier months are clustered in the left, middle months are clustered in the middle of the graph, and later months are clustered on the right. On the other hand, for the 2020 plot, the earlier months are clustered in the bottom right and later months are clustered in the top right.

# Q6

## Q6 (a)
Importing the data.
```{r, eval = TRUE}
dsrc <- DirSource("p5-data/sotu/files")
address <- Corpus(dsrc, readerControl=list(language="en"))
#summary(address)

party <- read.table("p5-data/sotu/party.txt", sep =",", header = F)
colnames(party) <- c("Party", "Pres", "Year")
filenames <- 1:231
for (i in filenames) {
  filenames[i] <-sprintf("a%d.txt", i)
}

party <- cbind(party, filenames = filenames)
View(party)
```

## Q6 (b)
Removing stopwords.
```{r, eval = TRUE}
stopwords <- read.table("p5-data/sotu/stopwords.txt", header = F)

#Change to stopwords.txt
address <- tm_map(address, removeWords, stopwords$V1)
```

## Q6 (c) (i)
Creating a term-document matrix. 
```{r, eval = TRUE}
tdm <- TermDocumentMatrix(address)
inspect(tdm[1:10,1:5])

mfw <- findMostFreqTerms(tdm[,1:5])
mfw

#need 3000 most freq words over all speeches, not including stop words
```

## Q6 (c) (ii)
Determining the party affiliations of the president using the Bernoulli model of NB. 
```{r, eval = TRUE}

```

## Q6 (c) (iii)
Determining the party affiliations of the president using the Multinomial model of NB. 
```{r, eval = TRUE}

```
